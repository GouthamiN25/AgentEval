# ğŸ§ª AgentEval : Human-Centric AI Agent Evaluation

AgentEval is a human-centric evaluation framework for AI agents that benchmarks decision quality, ethical reasoning, and failure handling â€” not just accuracy.

Instead of evaluating AI agents only on outputs, AgentEval evaluates how an agent thinks, similar to how humans are assessed in leadership or crisis scenarios.

ğŸ”— Live Demo:
ğŸ‘‰ https://agenteval-ee7rfvxr3zjbzwxavq7g7k.streamlit.app/

ğŸ¥ Demo Video:
ğŸ‘‰ https://drive.google.com/file/d/1yVt3EiMCGw4lrBOuyATtplaTgB8J-eul/view

## ğŸš€ Why AgentEval?

As AI systems become more agentic, traditional metrics like accuracy and latency are no longer enough.

Real-world AI agents must:

Make decisions under uncertainty

Balance ethics, business, and compliance

Handle bias and failure responsibly

Communicate trade-offs clearly

AgentEval was built to evaluate these human-level qualities in AI agents.

## ğŸ§  What AgentEval Does

AgentEval simulates high-stakes real-world scenarios and evaluates AI agents across five human-centric dimensions:


| Dimension                 | What It Measures                                      |
| ------------------------- | ----------------------------------------------------- |
| **Reasoning Quality**     | Clarity, structure, and depth of reasoning            |
| **Decision Consistency**  | Alignment between actions and stated constraints      |
| **Collaboration Mindset** | Stakeholder awareness and coordination                |
| **Bias Awareness**        | Recognition and mitigation of unfair outcomes         |
| **Failure Handling**      | Safety guardrails, rollback plans, and accountability |

Results are presented as:

Radar chart visualization

Dimensional score breakdown (0â€“100)

Natural-language judge rationale

Downloadable scorecard JSON

## âš™ï¸ How It Works (Architecture)
Scenario + Persona

        â†“
        
   AI Agent (Gemini)
   
        â†“
        
 Gemini-as-Judge
 
        â†“
        
 Structured Evaluation JSON
 
        â†“
        
 Scorecard + Visual Analytics
 

ğŸ”¹ Gemini-as-Judge

A separate Gemini model evaluates the agentâ€™s response â€” similar to how a human reviewer would assess leadership decisions.

ğŸ”¹ Demo Mode Fallback

If Gemini API quotas are exceeded, AgentEval automatically switches to Demo Mode, using cached high-quality outputs so:

The app remains usable

Demos never break

Judges always see results

## ğŸ§ª Example Scenarios

Chimera Recruitment Bias Crisis

AI recruitment system shows bias against a protected group under launch pressure.

Aether Mental Health Launch Crisis

Mental health chatbot shows unsafe confidence; safety vs growth trade-offs required.

Each scenario forces agents to balance ethics, business impact, and compliance.

## ğŸ–¥ï¸ Tech Stack

Frontend / App: Streamlit

LLMs: Google Gemini (agent + judge)

Visualization: Plotly

Data Handling: Pandas

Environment Management: python-dotenv

Deployment: Streamlit Community Cloud

## ğŸ“ Project Structure
AgentEval/

â”œâ”€â”€ app.py                # Main Streamlit application

â”œâ”€â”€ gemini_client.py      # Gemini API wrapper

â”œâ”€â”€ prompts.py            # Agent & evaluator prompts

â”œâ”€â”€ scoring.py            # JSON parsing & scoring logic

â”œâ”€â”€ storage.py            # Run persistence (local)

â”œâ”€â”€ requirements.txt      # Dependencies

â”œâ”€â”€ runtime.txt           # Python runtime for Streamlit

â”œâ”€â”€ .gitignore            # Secrets & env protection

## ğŸ” Environment Setup (Local)

Create a .env file in the project root:

GEMINI_API_KEY=your_key_here
GEMINI_MODEL=gemini-2.0-flash


âš ï¸ .env is ignored by git â€” never commit API keys.

â–¶ï¸ Run Locally
pip install -r requirements.txt
streamlit run app.py

ğŸŒ Live Deployment

The app is deployed on Streamlit Cloud:

ğŸ‘‰ https://agenteval-ee7rfvxr3zjbzwxavq7g7k.streamlit.app/

## ğŸ¯ What Makes AgentEval Different

âœ… Evaluates how an AI reasons, not just outputs

âœ… Uses LLM-as-Judge instead of hard-coded rules

âœ… Human-centric dimensions aligned with real leadership review

âœ… Reliable public demos via Demo Mode fallback

âœ… Transparent, explainable scorecards

## ğŸ”® Future Improvements

Multi-agent comparison matrix

Scenario authoring UI

Longitudinal agent performance tracking

Export to enterprise evaluation pipelines

Support for additional judge models

## ğŸ‘©â€ğŸ’» Author

Gouthami Nadupuri

Data Scientist | AI Engineer

GitHub: https://github.com/GouthamiN25
